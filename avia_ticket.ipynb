{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbcf4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook      #Устанавливает соединение с postgres\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook                #Устанавливает соединение с S3\n",
    "from airflow import DAG, models\n",
    "from airflow.decorators import dag, task\n",
    "import datetime as dt\n",
    "from airflow.operators.python import PythonOperator\n",
    "import logging  #Логирование\n",
    "import json #Сериализация и десириализация json в строку для загрузки в S3\n",
    "from io import StringIO #Говорит «Это не путь, а строка — обращайся с ней как с файлоподобным объектом»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3dabc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IATA_translate = {\n",
    "    'IKT': 'Иркутск',\n",
    "    'HKT': 'Пхукет',\n",
    "    'MOW': 'Москва',\n",
    "    'AER': 'Сочи',\n",
    "    'LED': 'Санкт_Петербург',\n",
    "    'VVO': 'Владивосток',\n",
    "    'MLE': 'Мале',\n",
    "    'DXB': 'Дубай',\n",
    "    'IST': 'Стамбул',\n",
    "    'AYT': 'Анталья',\n",
    "    'NHA': 'Нячанг',\n",
    "    'SGN': 'Хошимин',\n",
    "    'HAN': 'Ханой',\n",
    "    'TYO': 'Токио',\n",
    "    'KUL': 'Куала_Лумпур',\n",
    "    'PKX': 'Пекин_Дасин',\n",
    "    'PEK': 'Пекин_Шоуду' \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list = ['HKT', 'MOW', 'AER', 'LED', 'VVO', 'MLE', 'DXB', 'IST', 'AYT', 'NHA', 'SGN', 'HAN', 'TYO', 'KUL', 'PKX', 'PEK']                 #Города назначения\n",
    "origin_city = 'IKT'                                             #Город вылета\n",
    "routs = []\n",
    "\n",
    "for i in city_list:                                             #Генерирует комбинацию билетов туда-обратно\n",
    "    city_dickt = {}\n",
    "    city_dickt['origin'] = origin_city\n",
    "    city_dickt['destination'] = i\n",
    "    city_dickt['bd'] = f'{IATA_translate[origin_city]}_{IATA_translate[i]}_outbound'\n",
    "    routs.append(city_dickt)\n",
    "    \n",
    "    city_dickt = {}\n",
    "    city_dickt['origin'] = i\n",
    "    city_dickt['destination'] = origin_city\n",
    "    city_dickt['bd'] = f'{IATA_translate[i]}_{IATA_translate[origin_city]}_return'\n",
    "    routs.append(city_dickt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e27c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(data, key,bucket='airflare'):  #Функция загружает данные в S3, data-файл для загрузки, bucket - имя бакета, key - путь и название файла\n",
    "    logging.info('Создание подключения к S3')\n",
    "    hook = S3Hook(aws_conn_id='minio')    #Установка подключения к S3\n",
    "    \n",
    "    logging.info('Подключение установлено')\n",
    "    \n",
    "    if not hook.check_for_bucket(bucket):           #Проверяет есть ли бакет с таким именем\n",
    "        hook.create_bucket(bucket_name=bucket)\n",
    "        \n",
    "    logging.info('Загрузка данных в S3')\n",
    "    \n",
    "    key=f\"temp/{key}\"\n",
    "    \n",
    "    hook.load_string(                   #Загружает строку в S3\n",
    "        string_data=data,           #Сериализованные в строку данные\n",
    "        key=key,                    #Путь до файла в заданном бакете\n",
    "        bucket_name=bucket,         #Название бакета\n",
    "        replace=True\n",
    "    )\n",
    "    \n",
    "    logging.info('Загрузка завершена успешно')\n",
    "    \n",
    "    return {'bucket': bucket, 'key': key}       #Возвращает путь до файла и название бакета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_s3(bucket, key, aws_conn_id='minio'):  #Функция выгружает сериальизованный в строку файл из S3\n",
    "    logging.info('Создание подключения к S3')\n",
    "    conn = S3Hook(aws_conn_id=aws_conn_id)      #Задает переменную подключения где aws_conn_id это id подключения к S3 в UI airflow\n",
    "    \n",
    "    logging.info('Подключение установлено, загрузка данных из S3')\n",
    "    \n",
    "    content = conn.read_key(bucket_name=bucket, key=key)    #Читает сериализованный файл в заданном бакете по заданному пути\n",
    "    \n",
    "    data = json.loads(content)                  #Десериализует файл\n",
    "    logging.info('Данные выгружены из S3')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8114c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': dt.datetime(2024, 1, 1),\n",
    "    'retries': 2,\n",
    "    'retry_delay': dt.timedelta(minutes=1)\n",
    "}\n",
    "\n",
    "with DAG(dag_id='ETL_airflare_data', default_args=default_args, schedule_interval='0 12 * * *', catchup=False, tags=['cost']) as dag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410aaee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @task\n",
    "    def extract(rout):\n",
    "        origin = rout['origin']                     #Место вылета\n",
    "        destination = rout['destination']           #Горот назначения\n",
    "        headers = {'x-access-token': '167317b476e1808633c659a8bbb35b13'}\n",
    "\n",
    "        url = \"https://api.travelpayouts.com/v2/prices/latest\"\n",
    "\n",
    "        querystring = {\"currency\": \"rub\",\n",
    "                    \"origin\": f\"{origin}\",\n",
    "                    \"destination\": f\"{destination}\",\n",
    "                    'beginning_of_period': '2025-09-01',\n",
    "                    \"period_type\": \"year\",\n",
    "                    'one_way': True,\n",
    "                    \"page\": \"1\",\n",
    "                    \"limit\": \"1000\",\n",
    "                    \"show_to_affiliates\": \"true\",\n",
    "                    \"sorting\": \"price\",\n",
    "                    \"trip_class\": \"0\"}\n",
    "\n",
    "        logging.info(f'Начало извлечения данных для направления {origin}-{destination}')\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "        if response.status_code != 200:                         #Проверка на отклик API\n",
    "            logging.error('Ошибка при чтении данных из API')\n",
    "            raise ValueError('Ошибка')\n",
    "        \n",
    "        path = upload_to_s3(data = json.dumps(response.json()), key=f\"response_{rout['origin']}_{rout['destination']}.json\")      #Полученный ответ оборачивает в JSON затем сериализуется в строку и загружается по заданному пути в S3\n",
    "        \n",
    "        logging.info('Данные успешно сохранены во временное хранилище')\n",
    "        logging.info('Данные успешно извлечены и переданы в Xcom')\n",
    "        \n",
    "        return {'path': path, 'rout': rout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @task\n",
    "    def transform(info):                                       #Функция оставляет только нужные нам данные\n",
    "        path = info['path']\n",
    "        rout= info['rout']\n",
    "        logging.info('Данные приняты из Xcom, начат процесс очистки')\n",
    "\n",
    "        file = load_from_s3(bucket=path['bucket'], key=path['key'])     #Выгружает файл из S3 и десериализует его\n",
    "\n",
    "        df = pd.DataFrame(file['data'])\n",
    "        df = df[['depart_date', 'origin', 'destination', 'trip_class',\n",
    "                'value', 'gate', 'duration', 'distance', 'number_of_changes']]\n",
    "\n",
    "        logging.info('Преобразование завершено, начинается передача DF в S3')\n",
    "        \n",
    "        content = df.to_json(index=False, orient='records')                     #Преобразует DF в JSON\n",
    "        \n",
    "        path = upload_to_s3(content, bucket=path['bucket'], key=f\"df_{rout['origin']}_{rout['destination']}.json\")     #Загружает в S3\n",
    "        \n",
    "        logging.info('Данные успешно сохранены в S3, передаю их в Xcom')\n",
    "        \n",
    "        logging.info('данные успешно очищены и записаны в хранилище')\n",
    "        return {'path': path, 'rout': rout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @task\n",
    "    def load(info):                                        #Функция загружает данные в БД\n",
    "        \n",
    "        logging.info('Данные из Xcom успешно выгружены, начинаю выгружать данные из S3')\n",
    "        \n",
    "        path = info['path']                                 #Путь до файла в S3\n",
    "        rout= info['rout']                                  #Текущее направление\n",
    "        \n",
    "        content = load_from_s3(bucket=path['bucket'], key=path['key'])      #Выгружает из S3 файл который внутри содержит список словарей\n",
    "\n",
    "        df = pd.DataFrame(list(content))\n",
    "        logging.info('Данные получены из хранилища, начат процесс отправки в БД')\n",
    "\n",
    "        hook = PostgresHook(postgres_conn_id='postgres_airfare')  #Указывать имя в airflow UI при соединении с БД которое указано в yaml(postgres_conn_id= это id в aiflow UI)\n",
    "        engine = hook.get_sqlalchemy_engine()\n",
    "        \n",
    "        logging.info(f'Размер DataFrame: {df.shape}') #Выводит в логи размерность DF\n",
    "        logging.info(f'Типы данных:\\n{df.dtypes}')   #Выводит в логи типы данных каждого столбца DF\n",
    "\n",
    "        logging.info('Связь с БД установлена')\n",
    "        try:\n",
    "            df.to_sql(f'{rout['bd']}', engine, if_exists='append', index=False)      #Отправляет данные в заданную таблицу\n",
    "        except Exception as e:                                          #Логирует любую ошибку(исключение) и записывает ее в переменную\n",
    "            logging.error(f'Ошибка при загрузки в БД {e}')\n",
    "        logging.info('процесс ETL успешно завершен, данные загружены в БД')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f961c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    extract_args = extract.expand(rout=routs)\n",
    "    transform_args = transform.expand(info=extract_args)\n",
    "    load.expand(info=transform_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
