{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbcf4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook      #Устанавливает соединение с postgres\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook                #Устанавливает соединение с S3\n",
    "from airflow import DAG, models\n",
    "import datetime as dt\n",
    "from airflow.operators.python import PythonOperator\n",
    "import logging  #Логирование\n",
    "import json #Сериализация и десириализация json в строку для загрузки в S3\n",
    "from io import StringIO #Говорит «Это не путь, а строка — обращайся с ней как с файлоподобным объектом»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e27c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(data, bucket='airflare', key='temp/response.json'):  #Функция загружает данные в S3, data-файл для загрузки, bucket - имя бакета, key - путь и название файла\n",
    "    logging.info('Создание подключения к S3')\n",
    "    hook = S3Hook(aws_conn_id='minio')    #Установка подключения к S3\n",
    "    json_data = data         #Сериализует JSON в строку для передачи в S3(так как в S3 нельзя передавать объекты python)\n",
    "    \n",
    "    logging.info('Подключение установлено')\n",
    "    \n",
    "    if not hook.check_for_bucket(bucket):           #Проверяет есть ли бакет с таким именем\n",
    "        hook.create_bucket(bucket_name=bucket)\n",
    "        \n",
    "    logging.info('Загрузка данных в S3')\n",
    "    \n",
    "    hook.load_string(                   #Загружает строку в S3\n",
    "        string_data=json_data,      #Сериализованные в строку данные\n",
    "        key=key,                    #Путь до файла в заданном бакете\n",
    "        bucket_name=bucket,         #Название бакета\n",
    "        replace=True\n",
    "    )\n",
    "    \n",
    "    logging.info('Загрузка завершена успешно')\n",
    "    \n",
    "    return {'bucket': bucket, 'key': key}       #Возвращает путь до файла и название бакета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_s3(bucket, key, aws_conn_id='minio'):  #Функция выгружает сериальизованный в строку файл из S3\n",
    "    logging.info('Создание подключения к S3')\n",
    "    conn = S3Hook(aws_conn_id=aws_conn_id)      #Задает переменную подключения где aws_conn_id это id подключения к S3 в UI airflow\n",
    "    \n",
    "    logging.info('Подключение установлено, загрузка данных из S3')\n",
    "    \n",
    "    content = conn.read_key(bucket_name=bucket, key=key)    #Читает сериализованный файл в заданном бакете по заданному пути\n",
    "    \n",
    "    data = json.loads(content)                  #Десериализует файл\n",
    "    logging.info('Данные выгружены из S3')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe80941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(**context):\n",
    "    headers = {'x-access-token': '167317b476e1808633c659a8bbb35b13'}\n",
    "\n",
    "    url = \"https://api.travelpayouts.com/v2/prices/latest\"\n",
    "\n",
    "    querystring = {\"currency\": \"rub\",\n",
    "                   \"origin\": \"IKT\",\n",
    "                   \"destination\": \"HKT\",\n",
    "                   'beginning_of_period': '2025-09-01',\n",
    "                   \"period_type\": \"year\",\n",
    "                   'one_way': True,\n",
    "                   \"page\": \"1\",\n",
    "                   \"limit\": \"1000\",\n",
    "                   \"show_to_affiliates\": \"true\",\n",
    "                   \"sorting\": \"price\",\n",
    "                   \"trip_class\": \"0\"}\n",
    "\n",
    "    logging.info('Начало извлечения данных')\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "    if response.status_code != 200:                         #Проверка на отклик API\n",
    "        logging.error('Ошибка при чтении данных из API')\n",
    "        raise ValueError('Ошибка')\n",
    "\n",
    "    \n",
    "    path = upload_to_s3(json.dumps(response.json()), key='temp/response.json')      #Полученный ответ оборачивает в JSON затем сериализуется в строку и загружается по заданному пути в S3\n",
    "    \n",
    "    logging.info('Данные успешно сохранены во временное хранилище')\n",
    "\n",
    "    context['ti'].xcom_push(key='df_dirty', value=path)                     #При помощи Xcom мы отправляем в другой DAG данные о местоположении данных по API\n",
    "\n",
    "    logging.info('Данные успешно извлечены и переданы в Xcom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(**context):                                       #Функция оставляет только нужные нам данные\n",
    "    path = context['ti'].xcom_pull(key='df_dirty')              #Выгружает из Xcom путь к файлу по ключу key='df_dirty'\n",
    "\n",
    "    logging.info('Данные приняты из Xcom, начат процесс очистки')\n",
    "\n",
    "    file = load_from_s3(bucket=path['bucket'], key=path['key'])     #Выгружает файл из S3 и десериализует его\n",
    "\n",
    "    df = pd.DataFrame(file['data'])\n",
    "    df = df[['depart_date', 'origin', 'destination', 'trip_class',\n",
    "             'value', 'gate', 'duration', 'distance', 'number_of_changes']]\n",
    "\n",
    "    logging.info('Преобразование завершено, начинается передача DF в S3')\n",
    "    \n",
    "    content = df.to_json(index=False, orient='records')                     #Преобразует DF в JSON\n",
    "    \n",
    "    path = upload_to_s3(content, bucket=path['bucket'], key='temp/df.json')     #Загружает в S3\n",
    "    \n",
    "    logging.info('Данные успешно сохранены в S3, передаю их в Xcom')\n",
    "    \n",
    "    context['ti'].xcom_push(key='path', value=path)                         #Отправляет путь до сохраненного Файла\n",
    "\n",
    "    logging.info('данные успешно очищены и записаны в хранилище')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6ff66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(**context):                                        #Функция загружает данные в БД\n",
    "    path = context['ti'].xcom_pull(key='path')              #Выгружает путь до файла в бакете из Xcom\n",
    "    logging.info('Данные из Xcom успешно выгружены, начинаю выгружать данные из S3')\n",
    "    \n",
    "    content = load_from_s3(bucket=path['bucket'], key=path['key'])      #Выгружает из S3 файл который внутри содержит список словарей\n",
    "    df = pd.DataFrame(content)\n",
    "    logging.info('Данные получены из хранилища, начат процесс отправки в БД')\n",
    "\n",
    "    hook = PostgresHook(postgres_conn_id='postgres_airfare')  #Указывать имя при соединении с БД которое указано в yaml\n",
    "    engine = hook.get_sqlalchemy_engine()\n",
    "    \n",
    "    logging.info(f'Размер DataFrame: {df.shape}') #Выводит в логи размерность DF\n",
    "    logging.info(f'Типы данных:\\n{df.dtypes}')   #Выводит в логи типы данных каждого столбца DF\n",
    "\n",
    "    logging.info('Связь с БД установлена')\n",
    "    try:\n",
    "        df.to_sql('irk_hkt_year_stats', engine, if_exists='append', index=False)      #Отправляет данные в заданную таблицу\n",
    "    except Exception as e:                                          #Логирует любую ошибку(исключение) и записывает ее в переменную\n",
    "        logging.error(f'Ошибка при загрузки в БД {e}')\n",
    "    logging.info('процесс ETL успешно завершен, данные загружены в БД')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8114c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': dt.datetime(2024, 1, 1),\n",
    "    'retries': 2,\n",
    "    'retry_delay': dt.timedelta(minutes=1)\n",
    "}\n",
    "\n",
    "with DAG(dag_id='ETL_airflare_data', default_args=default_args, schedule_interval='0 12 * * *', catchup=False, tags=['cost']) as dag:\n",
    "    task_extract = PythonOperator(\n",
    "        task_id='extract',\n",
    "        python_callable=extract\n",
    "    )\n",
    "\n",
    "    task_transform = PythonOperator(\n",
    "        task_id='transform',\n",
    "        python_callable=transform\n",
    "    )\n",
    "\n",
    "    task_load = PythonOperator(\n",
    "        task_id='load',\n",
    "        python_callable=load\n",
    "    )\n",
    "\n",
    "    task_extract >> task_transform >> task_load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
