{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbcf4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook      #Устанавливает соединение с postgres\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook                #Устанавливает соединение с S3\n",
    "from airflow import DAG, models\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime, timedelta\n",
    "import logging  #Логирование\n",
    "import json #Сериализация и десириализация json в строку для загрузки в S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa9d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = PostgresHook(postgres_conn_id='postgres_airfare')                        #Извлечение IATA кодов из БД\n",
    "engine = hook.get_sqlalchemy_engine()\n",
    "iata_df = pd.read_sql('SELECT id, iata FROM iata_codes', con=engine)\n",
    "iata_dict = dict(zip(iata_df['iata'], iata_df['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list = list(iata_df['iata'])                 #Города назначения\n",
    "routs = []\n",
    "                                     \n",
    "for i in range(1, len(city_list)):                  #Генерирует комбинацию билетов туда-обратно\n",
    "    city_dict = {}\n",
    "    city_dict['origin'] = city_list[0]\n",
    "    city_dict['destination'] = city_list[i]\n",
    "    city_dict['bd'] = 'ikt_airflare_outbound'\n",
    "    routs.append(city_dict)\n",
    "    \n",
    "    city_dict = {}\n",
    "    city_dict['origin'] = city_list[i]\n",
    "    city_dict['destination'] = city_list[0]\n",
    "    city_dict['bd'] = 'ikt_airflare_return'\n",
    "    routs.append(city_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e27c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(data, key, bucket='airflare'):  #Функция загружает данные в S3, data-файл для загрузки, bucket - имя бакета, key - путь и название файла\n",
    "    logging.info('Создание подключения к S3')\n",
    "    hook = S3Hook(aws_conn_id='minio')    #Установка подключения к S3\n",
    "    \n",
    "    try:\n",
    "        if not hook.check_for_bucket(bucket):           #Проверяет есть ли бакет с таким именем\n",
    "            hook.create_bucket(bucket_name=bucket)\n",
    "        logging.info('Подключение установлено')\n",
    "    except Exception as e:                              #Логирование ошибки\n",
    "        logging.error(f'Ошибка подключения к minio, {e}')\n",
    "                \n",
    "    logging.info('Загрузка данных в S3')\n",
    "    \n",
    "    \n",
    "    hook.load_string(                   #Загружает строку в S3\n",
    "        string_data=data,           #Сериализованные в строку данные\n",
    "        key=key,                    #Путь до файла в заданном бакете\n",
    "        bucket_name=bucket,         #Название бакета\n",
    "        replace=True\n",
    "    )\n",
    "    \n",
    "    logging.info('Загрузка завершена успешно')\n",
    "    \n",
    "    return {'bucket': bucket, 'key': key}       #Возвращает путь до файла и название бакета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_s3(bucket, key, aws_conn_id='minio'):  #Функция выгружает сериальизованный в строку файл из S3\n",
    "    logging.info('Создание подключения к S3')\n",
    "    hook = S3Hook(aws_conn_id=aws_conn_id)      #Задает переменную подключения где aws_conn_id это id подключения к S3 в UI airflow\n",
    "    try:\n",
    "        if not hook.check_for_bucket(bucket):           #Проверяет есть ли бакет с таким именем\n",
    "            hook.create_bucket(bucket_name=bucket)\n",
    "        logging.info('Подключение установлено')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Ошибка подключения к minio, {e}')\n",
    "        \n",
    "    logging.info('Подключение установлено, загрузка данных из S3')\n",
    "    \n",
    "    content = hook.read_key(bucket_name=bucket, key=key)    #Читает сериализованный файл в заданном бакете по заданному пути\n",
    "    \n",
    "    data = json.loads(content)                  #Десериализует файл\n",
    "    logging.info('Данные выгружены из S3')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8114c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=1)\n",
    "}\n",
    "\n",
    "with DAG(dag_id='ETL_airflare_data', default_args=default_args, schedule_interval='0 12 * * *', catchup=False, tags=['cost']) as dag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410aaee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @task\n",
    "    def extract(rout):\n",
    "        origin = rout['origin']                     #Место вылета\n",
    "        destination = rout['destination']           #Горот назначения\n",
    "        headers = {'x-access-token': '167317b476e1808633c659a8bbb35b13'}\n",
    "\n",
    "        url = \"https://api.travelpayouts.com/v2/prices/latest\"\n",
    "\n",
    "        querystring = {\"currency\": \"rub\",\n",
    "                    \"origin\": f\"{origin}\",\n",
    "                    \"destination\": f\"{destination}\",\n",
    "                    'beginning_of_period': '2025-09-01',\n",
    "                    \"period_type\": \"year\",\n",
    "                    'one_way': True,\n",
    "                    \"page\": \"1\",\n",
    "                    \"limit\": \"1000\",\n",
    "                    \"show_to_affiliates\": \"true\",\n",
    "                    \"sorting\": \"price\",\n",
    "                    \"trip_class\": \"0\"}\n",
    "\n",
    "        logging.info(f'Начало извлечения данных для направления {origin}-{destination}')\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "        if response.status_code != 200:                         #Проверка на отклик API\n",
    "            logging.error('Ошибка при чтении данных из API')\n",
    "            raise ValueError('Ошибка')\n",
    "        \n",
    "        path = upload_to_s3(data = json.dumps(response.json()), key=f\"row_airflare_data_by_day/response_{rout['origin']}_{rout['destination']}_{str(datetime.today().date())}.json\")      #Полученный ответ оборачивает в JSON затем сериализуется в строку и загружается по заданному пути в S3\n",
    "        \n",
    "        logging.info('Данные успешно сохранены в S3')\n",
    "        \n",
    "        return {'path': path, 'rout': rout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @task\n",
    "    def transform(info):                                       #Функция оставляет только нужные нам данные\n",
    "        path = info['path']\n",
    "        rout= info['rout']\n",
    "        logging.info('Извлечение данных из S3, подготовка к обработке')\n",
    "\n",
    "        file = load_from_s3(bucket=path['bucket'], key=path['key'])     #Выгружает файл из S3 и десериализует его\n",
    "\n",
    "        logging.info('Начата обработка данных')\n",
    "        \n",
    "        df = pd.DataFrame(file['data'])\n",
    "        df = df[['depart_date', 'origin', 'destination', 'trip_class',\n",
    "                'value', 'gate', 'duration', 'distance', 'number_of_changes']]\n",
    "        \n",
    "        cur_date = pd.Series([datetime.date(datetime.today())] * len(df), name='date_of_extraction').astype('str')      #Создает серию хранящую текущую дату\n",
    "        \n",
    "        day_before_departure = pd.to_datetime(df['depart_date']) - pd.to_datetime(cur_date)         #Создает серию хранящую время до вылета\n",
    "        day_before_departure.name = 'day_before_departure'\n",
    "        day_before_departure = day_before_departure.map(lambda x: x.days)\n",
    "              \n",
    "\n",
    "        df = pd.concat([df, cur_date, day_before_departure], axis=1)\n",
    "              \n",
    "        df['origin'] = df['origin'].map(iata_dict)                   #замена IATA кодов на индексы для нормализации данных\n",
    "        df['destination'] = df['destination'].map(iata_dict)\n",
    "        upload_to_s3(df.to_json(index=False, orient='records'), 'df_now.json')\n",
    "        logging.info(df['date_of_extraction'].dtype)\n",
    "        logging.info(df['depart_date'].dtype)\n",
    "        \n",
    "        \n",
    "        logging.info('Обработка завершена, начинается передача DF в S3')\n",
    "        \n",
    "        content = df.to_json(index=False, orient='records')                     #Преобразует DF в JSON\n",
    "        \n",
    "        path = upload_to_s3(content, bucket=path['bucket'], key=f\"DataFrames/df_{rout['origin']}_{rout['destination']}.json\")     #Загружает в S3\n",
    "        \n",
    "        logging.info('Данные успешно сохранены в S3')\n",
    "        \n",
    "        return {'path': path, 'rout': rout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @task\n",
    "    def load_to_postgres(info):                                        #Функция загружает данные в БД       \n",
    "        path = info['path']                                 #Путь до файла в S3\n",
    "        rout= info['rout']                                  #Текущее направление\n",
    "        content = load_from_s3(bucket=path['bucket'], key=path['key'])      #Выгружает из S3 файл который внутри содержит список словарей\n",
    "        \n",
    "        logging.info('Данные получены из хранилища, начат процесс отправки в БД')\n",
    "\n",
    "        df = pd.DataFrame(list(content))\n",
    "\n",
    "        hook = PostgresHook(postgres_conn_id='postgres_airfare')  #Указывать имя в airflow UI при соединении с БД которое указано в yaml(postgres_conn_id= это id в aiflow UI)\n",
    "        engine = hook.get_sqlalchemy_engine()\n",
    "        logging.info('Проверяю подключение к postgres')\n",
    "        try:                                #Проверяем подключение к postgres\n",
    "            # Получаем подключение\n",
    "            conn = hook.get_conn()\n",
    "    \n",
    "            # Создаем курсор для выполнения простого sql запроса для проверки соединения\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(\"SELECT 1;\")         #Выполняет запрос к БД\n",
    "                result = cursor.fetchone()          #Возвращает результат запроса\n",
    "                print(\"Соединение с PostgreSQL успешно. Результат запроса:\", result)\n",
    "        except Exception as e:\n",
    "            print(\"Ошибка соединения с PostgreSQL:\", e)\n",
    "        \n",
    "        \n",
    "        \n",
    "        logging.info(f'Размер DataFrame: {df.shape}') #Выводит в логи размерность DF\n",
    "        logging.info(f'Типы данных:\\n{df.dtypes}')   #Выводит в логи типы данных каждого столбца DF\n",
    "\n",
    "        try:\n",
    "            df.to_sql(f'{rout['bd']}', engine, if_exists='append', index=False)      #Отправляет данные в заданную таблицу\n",
    "        except Exception as e:                                          #Логирует любую ошибку(исключение) и записывает ее в переменную\n",
    "            logging.error(f'Ошибка при загрузки в БД {e}')\n",
    "        logging.info('процесс ETL успешно завершен, данные загружены в БД')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f961c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    extract_args = extract.expand(rout=routs)\n",
    "    transform_args = transform.expand(info=extract_args)\n",
    "    load_to_postgres.expand(info=transform_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
